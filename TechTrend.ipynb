{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Moch. Udhay Yunussabil\t            163221004\n",
    "2. Valiantino Ramandhika A.             163221038\n",
    "3. Patrick Andrasena Tumengkol          163221077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/datasets/sumantindurkhya/phoronix-tech-news-articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechNewsAnalyzer:\n",
    "    def __init__(self, csv_path=\"phoronix.csv\"):\n",
    "        \"\"\"Initialize the Technology News Analysis System\"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.lemmatizer = nltk.WordNetLemmatizer()\n",
    "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        self.vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        \n",
    "        # Technology categories for classification\n",
    "        self.tech_categories = {\n",
    "            'AI/ML': ['artificial intelligence', 'machine learning', 'deep learning', 'neural network', \n",
    "                     'ai', 'ml', 'algorithm', 'automation'],\n",
    "            'Hardware': ['cpu', 'gpu', 'processor', 'hardware', 'intel', 'amd', 'nvidia'],\n",
    "            'Software': ['software', 'application', 'program', 'code', 'development'],\n",
    "            'Security': ['security', 'hack', 'breach', 'malware', 'ransomware', 'encryption'],\n",
    "            'Cloud': ['cloud', 'aws', 'azure', 'google cloud', 'saas', 'paas', 'iaas'],\n",
    "            'Mobile': ['mobile', 'smartphone', 'ios', 'android', 'app', 'mobile app'],\n",
    "            'Gaming': ['game', 'gaming', 'console', 'playstation', 'xbox', 'nintendo']\n",
    "        }\n",
    "\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and prepare the dataset\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.csv_path)\n",
    "            print(f\"Successfully loaded {len(self.df)} articles from {self.csv_path}\")\n",
    "            \n",
    "            # Convert date column to datetime\n",
    "            if 'created_at' in self.df.columns:\n",
    "                self.df['created_at'] = pd.to_datetime(self.df['created_at'], errors='coerce')\n",
    "            \n",
    "            # Fill missing values\n",
    "            self.df['title'] = self.df['title'].fillna('')\n",
    "            self.df['text'] = self.df['text'].fillna('')\n",
    "            \n",
    "            print(\"Data preparation completed.\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Basic text preprocessing\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "            \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs and special patterns\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        # Tokenize and remove stopwords\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in self.stop_words and len(word) > 2]\n",
    "        \n",
    "        # Lemmatization\n",
    "        processed_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring title terhadap kategori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_category(self, text: str) -> str:\n",
    "        \"\"\"Detect the primary technology category\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        category_scores = {}\n",
    "        \n",
    "        for category, keywords in self.tech_categories.items():\n",
    "            score = sum(1 for keyword in keywords if keyword.lower() in text_lower)\n",
    "            if score > 0:\n",
    "                category_scores[category] = score\n",
    "        \n",
    "        if category_scores:\n",
    "            return max(category_scores, key=category_scores.get)\n",
    "        return 'General'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis Trend (TF-IDF Matrix + K-Means Clustering) + save result package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trends(self, num_clusters: int = 5):\n",
    "        \"\"\"Analyze trends and create visualizations\"\"\"\n",
    "        if self.df.empty:\n",
    "            print(\"No data available for analysis\")\n",
    "            return\n",
    "        \n",
    "        # Process all titles\n",
    "        processed_titles = [self.preprocess_text(title) for title in self.df['title']]\n",
    "        \n",
    "        # Create TF-IDF matrix\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(processed_titles)\n",
    "        \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "        \n",
    "        # Add categories and clusters to dataframe\n",
    "        self.df['category'] = self.df['title'].apply(self.detect_category)\n",
    "        self.df['cluster'] = cluster_labels\n",
    "        \n",
    "        # Create timestamp for this analysis\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        analysis_dir = f\"analysis_results_{timestamp}\"\n",
    "        os.makedirs(analysis_dir, exist_ok=True)\n",
    "\n",
    "        # 1. Category Distribution Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        category_counts = self.df['category'].value_counts()\n",
    "        sns.barplot(x=category_counts.index, y=category_counts.values)\n",
    "        plt.title('Distribution of Technology Categories')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Number of Articles')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(analysis_dir, 'category_distribution.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 2. Cluster Analysis Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        cluster_counts = self.df['cluster'].value_counts().sort_index()\n",
    "        sns.barplot(x=cluster_counts.index, y=cluster_counts.values)\n",
    "        plt.title('Distribution of Article Clusters')\n",
    "        plt.xlabel('Cluster ID')\n",
    "        plt.ylabel('Number of Articles')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(analysis_dir, 'cluster_distribution.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Save analysis results\n",
    "        results_file = os.path.join(analysis_dir, 'trend_analysis.txt')\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"Technology News Trend Analysis\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            \n",
    "            # Category Analysis\n",
    "            f.write(\"Category Distribution:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            for category, count in category_counts.items():\n",
    "                f.write(f\"{category}: {count} articles\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Cluster Analysis\n",
    "            f.write(\"Cluster Analysis:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            for cluster_id in range(num_clusters):\n",
    "                cluster_articles = self.df[self.df['cluster'] == cluster_id]\n",
    "                f.write(f\"\\nCluster {cluster_id} ({len(cluster_articles)} articles):\\n\")\n",
    "                \n",
    "                # Get top titles for this cluster\n",
    "                top_titles = cluster_articles['title'].head(5)\n",
    "                for i, title in enumerate(top_titles, 1):\n",
    "                    f.write(f\"{i}. {title}\\n\")\n",
    "                \n",
    "                # Get category distribution within cluster\n",
    "                cluster_categories = cluster_articles['category'].value_counts()\n",
    "                f.write(\"\\nCategory distribution in this cluster:\\n\")\n",
    "                for category, count in cluster_categories.items():\n",
    "                    f.write(f\"  - {category}: {count} articles\\n\")\n",
    "        \n",
    "        print(f\"\\nAnalysis results have been saved in directory: {analysis_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inisialisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the technology news analysis\"\"\"\n",
    "    print(\" Initializing Technology News Analysis System...\")\n",
    "    print(\" Analyzing trends from Phoronix dataset\")\n",
    "    print()\n",
    "    \n",
    "    analyzer = TechNewsAnalyzer()\n",
    "    analyzer.analyze_trends()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Check the generated files for detailed results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
